{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "513acd5b",
      "metadata": {
        "id": "513acd5b"
      },
      "source": [
        "\n",
        "# ğŸ¥ MediaEval-Medico-2025 â€” Subtask 1: GI Image VQA (Colab/T4 Friendly)\n",
        "\n",
        "This notebook fine-tunes **`google/paligemma-3b-pt-224`** on **Kvasir-VQA-x1** using **[ms-swift](https://swift.readthedocs.io/)**, then pushes the result to **Hugging Face Hub**.  \n",
        "Itâ€™s optimized for the **free Colab T4 GPU** tier (â‰ˆ16â€¯GB) with 4-bit quantization + LoRA.\n",
        "\n",
        "**Repo:** ğŸŒ MediaEval-Medico-2025 â€” https://github.com/simula/MediaEval-Medico-2025\n",
        "\n",
        "\n",
        "**What youâ€™ll get**\n",
        "- âœ… Data prep (images + JSONL suitable for ms-swift VLMs)\n",
        "- âœ… T4-friendly training config (QLoRA + LoRA + checkpointing)\n",
        "- âœ… Validation during training\n",
        "- âœ… Auto-push to Hugging Face Hub\n",
        "- âœ… Minimal inference sanity-check\n",
        "\n",
        "> **Tip:** Tune `num_train_epochs`, batch size, and learning rate based on your GPU memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea6cbb3",
      "metadata": {
        "id": "7ea6cbb3"
      },
      "source": [
        "## ğŸ”§ Runtime & GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4747d8d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4747d8d3",
        "outputId": "6a677e41-54f0-4222-f757-41d60197829b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Make sure you're on Colab with GPU: Runtime â†’ Change runtime type â†’ T4 GPU\n",
        "import torch, platform, sys, subprocess, json\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print(\"âš ï¸ No GPU detected. Please enable a T4 GPU in Colab runtime.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f4c40d",
      "metadata": {
        "id": "75f4c40d"
      },
      "source": [
        "## ğŸ“¦ Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b061f5ef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b061f5ef",
        "outputId": "1e4b19be-c2fe-4942-ab0d-c8067f1a507a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ms-swift\n",
            "  Downloading ms_swift-3.10.0-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from ms-swift) (1.11.0)\n",
            "Collecting addict (from ms-swift)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from ms-swift) (3.13.2)\n",
            "Collecting attrdict (from ms-swift)\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting binpacking (from ms-swift)\n",
            "  Downloading binpacking-1.5.2.tar.gz (8.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from ms-swift) (3.4.4)\n",
            "Collecting cpm-kernels (from ms-swift)\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dacite (from ms-swift)\n",
            "  Downloading dacite-1.9.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting datasets<4.0,>=3.0 (from ms-swift)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.8.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.121.1)\n",
            "Requirement already satisfied: gradio>=3.40.0 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (5.49.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from ms-swift) (8.7.0)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.42.1)\n",
            "Collecting json-repair (from ms-swift)\n",
            "  Downloading json_repair-0.53.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from ms-swift) (3.10.0)\n",
            "Collecting modelscope>=1.23 (from ms-swift)\n",
            "  Downloading modelscope-1.31.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from ms-swift) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.0.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.3.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (from ms-swift) (1.109.1)\n",
            "Collecting oss2 (from ms-swift)\n",
            "  Downloading oss2-2.19.1.tar.gz (298 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.8/298.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.2.2)\n",
            "Requirement already satisfied: peft<0.18,>=0.11 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.17.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from ms-swift) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.32.4)\n",
            "Collecting rouge (from ms-swift)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.6.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from ms-swift) (1.16.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.2.1)\n",
            "Requirement already satisfied: simplejson>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (3.20.2)\n",
            "Requirement already satisfied: sortedcontainers>=1.5.9 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.4.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from ms-swift) (2.19.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from ms-swift) (4.67.1)\n",
            "Requirement already satisfied: transformers<4.58,>=4.33 in /usr/local/lib/python3.12/dist-packages (from ms-swift) (4.57.1)\n",
            "Collecting transformers-stream-generator (from ms-swift)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trl<0.25,>=0.15 (from ms-swift)\n",
            "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.38.0)\n",
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.12/dist-packages (from ms-swift) (0.25.0)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0,>=3.0->ms-swift) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets<4.0,>=3.0->ms-swift) (0.36.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (1.2.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (3.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.14.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.49.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio>=3.40.0->ms-swift) (0.20.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio>=3.40.0->ms-swift) (15.0.1)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi->ms-swift) (0.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.23->ms-swift) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.12/dist-packages (from modelscope>=1.23->ms-swift) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->ms-swift) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->ms-swift) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->ms-swift) (2025.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft<0.18,>=0.11->ms-swift) (5.9.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->ms-swift) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->ms-swift) (2025.10.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<4.58,>=4.33->ms-swift) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<4.58,>=4.33->ms-swift) (0.22.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn->ms-swift) (0.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->ms-swift) (1.22.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from attrdict->ms-swift) (1.17.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from binpacking->ms-swift) (1.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->ms-swift) (3.23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ms-swift) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ms-swift) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ms-swift) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ms-swift) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->ms-swift) (3.2.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->ms-swift) (1.5.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->ms-swift) (4.9.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai->ms-swift) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai->ms-swift) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai->ms-swift) (1.3.1)\n",
            "Collecting crcmod>=1.7 (from oss2->ms-swift)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome>=3.4.7 (from oss2->ms-swift)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting aliyun-python-sdk-kms>=2.4.1 (from oss2->ms-swift)\n",
            "  Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting aliyun-python-sdk-core>=2.13.12 (from oss2->ms-swift)\n",
            "  Downloading aliyun-python-sdk-core-2.16.0.tar.gz (449 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m449.6/449.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->ms-swift) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->ms-swift) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->ms-swift) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->ms-swift) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->ms-swift) (3.1.3)\n",
            "Collecting jmespath<1.0.0,>=0.9.3 (from aliyun-python-sdk-core>=2.13.12->oss2->ms-swift)\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: cryptography>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from aliyun-python-sdk-core>=2.13.12->oss2->ms-swift) (43.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio>=3.40.0->ms-swift) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets<4.0,>=3.0->ms-swift) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=3.40.0->ms-swift) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio>=3.40.0->ms-swift) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->ms-swift) (2.0.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.40.0->ms-swift) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.40.0->ms-swift) (2.19.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.0.0->aliyun-python-sdk-core>=2.13.12->oss2->ms-swift) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=3.40.0->ms-swift) (0.1.2)\n",
            "Downloading ms_swift-3.10.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m926.8/926.8 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl (59.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.31.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m144.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dacite-1.9.2-py3-none-any.whl (16 kB)\n",
            "Downloading json_repair-0.53.0-py3-none-any.whl (27 kB)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Downloading aliyun_python_sdk_kms-2.16.5-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m107.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: binpacking, oss2, transformers-stream-generator, aliyun-python-sdk-core, crcmod\n",
            "  Building wheel for binpacking (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for binpacking: filename=binpacking-1.5.2-py3-none-any.whl size=10092 sha256=f150b00692433de2b4aa085e93ea208e183e4e326a46d55279f05dc3eda1c199\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f8/d0/c5bbc1cf7ba3d46470688acfb18fbee2c0baa81696cd5df7b9\n",
            "  Building wheel for oss2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for oss2: filename=oss2-2.19.1-py3-none-any.whl size=123940 sha256=5833e73f26fb748e43b58475ff03c576358d5184888f7dd513c12d0c5557909d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/aa/ce/a0c9e73f8e4a3b7813b6b0d9dbddfb83028fd416828f27d97b\n",
            "  Building wheel for transformers-stream-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12426 sha256=67a535ccbb13bf745ef7ae5e1f57223d74c67b0834ef8a43d4e949331206b342\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/58/d2/014cb67c3cc6def738c1b1635dbf4e3dab6fb63aba7070dce0\n",
            "  Building wheel for aliyun-python-sdk-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aliyun-python-sdk-core: filename=aliyun_python_sdk_core-2.16.0-py3-none-any.whl size=535315 sha256=b5116426ed6bc0776bfeb83aaea933d44985611ad2546a052e11237100bee522\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/60/7c/80d5fdcd6d0e016e4a7ff4a66fd9321b3096ae676d78fe0212\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp312-cp312-linux_x86_64.whl size=31833 sha256=e8f3b1c9bb886b8b0ba9c8e5959f715140f4d95102c14921e3220fd6a6a9a2e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/08/0b/caa8b1380122cbfe6a03eaccbec0f63c67e619af4e30ca5e2a\n",
            "Successfully built binpacking oss2 transformers-stream-generator aliyun-python-sdk-core crcmod\n",
            "Installing collected packages: crcmod, cpm-kernels, addict, rouge, pycryptodome, json-repair, jmespath, dacite, binpacking, attrdict, modelscope, aliyun-python-sdk-core, datasets, bitsandbytes, aliyun-python-sdk-kms, trl, transformers-stream-generator, oss2, ms-swift\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "Successfully installed addict-2.4.0 aliyun-python-sdk-core-2.16.0 aliyun-python-sdk-kms-2.16.5 attrdict-2.0.1 binpacking-1.5.2 bitsandbytes-0.48.2 cpm-kernels-1.0.11 crcmod-1.7 dacite-1.9.2 datasets-3.6.0 jmespath-0.10.0 json-repair-0.53.0 modelscope-1.31.0 ms-swift-3.10.0 oss2-2.19.1 pycryptodome-3.23.0 rouge-1.0.1 transformers-stream-generator-0.0.5 trl-0.24.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install ms-swift bitsandbytes wandb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef8138e6",
      "metadata": {
        "id": "ef8138e6"
      },
      "source": [
        "\n",
        "## ğŸ” Authenticate\n",
        "- **Hugging Face**: Required to push your model to Hub. Create a [token](https://huggingface.co/settings/tokens) with `write` scope.\n",
        "- **Weights & Biases (optional)**: Set a project name to log metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7641f6fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7641f6fd",
        "outputId": "1dcb4b08-a667-4f8d-fd6b-44a87f27a351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/hf\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/cli/hf.py\", line 59, in main\n",
            "    service.run()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/cli/auth.py\", line 126, in run\n",
            "    login(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_login.py\", line 124, in login\n",
            "    interpreter_login(new_session=new_session)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_deprecation.py\", line 31, in inner_f\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/_login.py\", line 281, in interpreter_login\n",
            "    token = getpass(\"Enter your token (input will not be visible): \")\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/getpass.py\", line 77, in unix_getpass\n",
            "    passwd = _raw_input(prompt, stream, input=input)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/getpass.py\", line 146, in _raw_input\n",
            "    line = input.readline()\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Abort",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAbort\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3436802949.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hf auth login --add-to-git-credential'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Kvasir-VQA-x1_Subtask1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \"\"\"\n\u001b[1;32m     79\u001b[0m     \u001b[0m_handle_host_wandb_setting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     logged_in, _ = _login(\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify, referrer, update_api_key, _silent, _disable_warning)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mkey_is_pre_configured\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    236\u001b[0m     ) -> Tuple[Optional[str], ApiKeyStatus]:\n\u001b[1;32m    237\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             directive = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self, referrer)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    215\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local, referrer)\u001b[0m\n\u001b[1;32m    184\u001b[0m                 )\n\u001b[1;32m    185\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_api_key_prompt_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferrer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_ask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOGIN_CHOICE_NOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;31m# TODO: Needs refactor as this needs to be handled by caller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt\u001b[0;34m(text, default, hide_input, confirmation_prompt, type, value_proc, prompt_suffix, show_default, err, show_choices)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/click/termui.py\u001b[0m in \u001b[0;36mprompt_func\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhide_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mecho\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAbort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue_proc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAbort\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from huggingface_hub import whoami, login\n",
        "import wandb, os\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "!hf auth login --add-to-git-credential\n",
        "wandb.login()\n",
        "\n",
        "os.environ[\"WANDB_PROJECT\"] = \"Kvasir-VQA-x1_Subtask1\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"false\"\n",
        "HF_USER = whoami()[\"name\"]\n",
        "print(\"Logged into HF as:\", HF_USER)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7163a9db",
      "metadata": {
        "id": "7163a9db"
      },
      "source": [
        "\n",
        "## ğŸ—‚ï¸ Data Preparation (Kvasir-VQA-x1)\n",
        "Weâ€™ll:\n",
        "1) Cache all images locally (once) from **`SimulaMet-HOST/Kvasir-VQA`**.  \n",
        "2) Build **VLM-ready JSONL** files (`messages` + `<image>` + `images` path) for **`SimulaMet/Kvasir-VQA-x1`** train/test splits.\n",
        "\n",
        "\n",
        "Remember, you also can add your data augmentation scripts  to augment images  or question, answers here.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0560efe",
      "metadata": {
        "id": "d0560efe"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import json, os\n",
        "\n",
        "# Working directories\n",
        "BASE_DIR = Path(\"./\")\n",
        "DATA_DIR = BASE_DIR / \"Kvasir-VQA-x1\"\n",
        "IMG_DIR  = DATA_DIR / \"images\"\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Data dir:\", DATA_DIR)\n",
        "print(\"Images dir:\", IMG_DIR)\n",
        "\n",
        "# 1) Save unique images locally\n",
        "print(\"â¬ Caching images from SimulaMet-HOST/Kvasir-VQA ...\")\n",
        "host = load_dataset(\"SimulaMet-HOST/Kvasir-VQA\", split=\"raw\")\n",
        "df = host.select_columns(['source', 'question', 'answer', 'img_id']).to_pandas()\n",
        "# Save one image per unique img_id\n",
        "for i, row in tqdm(df.groupby('img_id').nth(0).iterrows(), total=df['img_id'].nunique()):\n",
        "    p = IMG_DIR / f\"{row['img_id']}.jpg\"\n",
        "    if p.exists():\n",
        "        continue\n",
        "    host[i]['image'].save(p)\n",
        "\n",
        "# 2) Create JSONLs for train/test from Kvasir-VQA-x1 (VLM-ready for ms-swift)\n",
        "print(\"Creating JSONLs ...\")\n",
        "def write_jsonl(split):\n",
        "    out_path = DATA_DIR / f\"Kvasir-VQA-x1-{split}.jsonl\"\n",
        "    ds = load_dataset(\"SimulaMet/Kvasir-VQA-x1\", split=split)\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in ds:\n",
        "            rec = {\n",
        "                \"messages\": [\n",
        "                    {\"role\": \"user\", \"content\": f\"<image>{r['question']}\"},\n",
        "                    {\"role\": \"assistant\", \"content\": r[\"answer\"]}\n",
        "                ],\n",
        "                \"images\": [str(IMG_DIR / f\"{r['img_id']}.jpg\")]\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    return out_path\n",
        "\n",
        "train_jsonl = write_jsonl(\"train\")\n",
        "test_jsonl  = write_jsonl(\"test\")\n",
        "\n",
        "print(\"Train JSONL:\", train_jsonl)\n",
        "print(\"Test  JSONL:\", test_jsonl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2127d7eb",
      "metadata": {
        "id": "2127d7eb"
      },
      "source": [
        "### ğŸ” Quick sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "965bd07e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "965bd07e",
        "outputId": "0520a6d4-3416-4608-ea9a-d95604c11b87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample 1 ---\n",
            "messages: [{'role': 'user', 'content': '<image>Are there any abnormalities, polyps, or anatomical landmarks visible in the image?'}, {'role': 'assistant', 'content': 'Evidence of oesophagitis is present with no polyps identified and the z-line is visible.'}]\n",
            "images: ['Kvasir-VQA-x1/images/clb0kvxvm90y4074yf50vf5nq.jpg']\n",
            "\n",
            "--- Sample 2 ---\n",
            "messages: [{'role': 'user', 'content': '<image>What procedure is depicted in the image and what type of polyp is observed?'}, {'role': 'assistant', 'content': 'Evidence of a colonoscopy with a paris iia polyp noted'}]\n",
            "images: ['Kvasir-VQA-x1/images/cl8k2u1r71foz083278j63qnm.jpg']\n",
            "\n",
            "--- Sample 3 ---\n",
            "messages: [{'role': 'user', 'content': '<image>Have all polyps been removed, is there any text present, and where is the abnormality located in the image?'}, {'role': 'assistant', 'content': 'Polyps remain present, text is visible, and the abnormality is located in the central and upper-center regions.'}]\n",
            "images: ['Kvasir-VQA-x1/images/cl8k2u1qa1ekz08324rek2qcv.jpg']\n",
            "\n",
            "Looks good âœ…\n"
          ]
        }
      ],
      "source": [
        "import json, itertools\n",
        "from pathlib import Path\n",
        "\n",
        "sample_lines = list(itertools.islice(open(train_jsonl, \"r\", encoding=\"utf-8\"), 3))\n",
        "for i, line in enumerate(sample_lines, 1):\n",
        "    j = json.loads(line)\n",
        "    print(f\"\\n--- Sample {i} ---\")\n",
        "    print(\"messages:\", j[\"messages\"])\n",
        "    print(\"images:\", j[\"images\"])\n",
        "    assert Path(j[\"images\"][0]).exists(), \"Missing image file!\"\n",
        "print(\"\\nLooks good âœ…\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "âš ï¸ âš ï¸ âš ï¸  \n",
        "To reduce validation time, we randomly sampled 1,000 entries from the full test set using the shuf command:"
      ],
      "metadata": {
        "id": "X3Br3uwxEge7"
      },
      "id": "X3Br3uwxEge7"
    },
    {
      "cell_type": "code",
      "source": [
        "!shuf -n 1000 Kvasir-VQA-x1/Kvasir-VQA-x1-test.jsonl > Kvasir-VQA-x1/Kvasir-VQA-x1-test-1000.jsonl\n",
        "VAL_1000_PATH= \"Kvasir-VQA-x1/Kvasir-VQA-x1-test-1000.jsonl\""
      ],
      "metadata": {
        "id": "4JBUv5xjEYYi"
      },
      "id": "4JBUv5xjEYYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ğŸš€ Fine-tune PaliGemma 3B (QLoRA + LoRA)\n",
        "> You can also use any other multimodal models listed here:  \n",
        "> https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html"
      ],
      "metadata": {
        "id": "iB0nDX4qDKFU"
      },
      "id": "iB0nDX4qDKFU"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME=\"google/paligemma-3b-pt-224\" ## you can choose this from the link above\n",
        "# Your target Huggingface repo name (can change as required!)\n",
        "HUB_MODEL_ID = f\"Kvasir-VQA-x1-lora_{datetime.now().strftime('%y%m%d-%H%M')}\" # appends date time at end\n",
        "\n",
        "TRAIN_PATH=str(train_jsonl)\n",
        "VAL_PATH=str(test_jsonl)\n",
        "\n",
        "print(\"Model:      \", MODEL_NAME)\n",
        "print(\"Train file: \", TRAIN_PATH)\n",
        "print(\"Valid file: \", VAL_PATH)\n",
        "print(\"Hub repo:   \", HUB_MODEL_ID)\n",
        "\n",
        "print(\"ğŸ“ You can find training logs after the training starts at: https://wandb.ai/home\")\n",
        "print(\"ğŸ“Œ After each validation stage, the HF repository will be updated with the best model.\")\n",
        "print(f\"âœ… Model will be available at: https://huggingface.co/{HF_USER}/{HUB_MODEL_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdkrDgcDGeJ",
        "outputId": "9abdd147-3b03-4e4e-c58a-bcfe7319a10a"
      },
      "id": "_VdkrDgcDGeJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:       google/paligemma-3b-pt-224\n",
            "Train file:  Kvasir-VQA-x1/Kvasir-VQA-x1-train.jsonl\n",
            "Valid file:  Kvasir-VQA-x1/Kvasir-VQA-x1-test.jsonl\n",
            "Hub repo:    Kvasir-VQA-x1-lora_250812-1155\n",
            "ğŸ“ You can find training logs after the training starts at: https://wandb.ai/home\n",
            "ğŸ“Œ After each validation stage, the HF repository will be updated with the best model.\n",
            "âœ… Model will be available at: https://huggingface.co/SushantGautam/Kvasir-VQA-x1-lora_250812-1155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf520529",
      "metadata": {
        "id": "bf520529"
      },
      "source": [
        "\n",
        "T4-friendly defaults for 3B:\n",
        "- `bnb` 4-bit quantization (nf4 + double quant)\n",
        "- `per_device_train_batch_size=4` (adjust if OOM)\n",
        "- `gradient_accumulation_steps=4` (effective batch â‰ˆ16)\n",
        "- `freeze_vit=true`, `gradient_checkpointing=true`\n",
        "\n",
        "> Increase batch size and/or `num_train_epochs` if you have more VRAM.\n",
        "\n",
        "See https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html for all supported training parameters. Play with them to get the best results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e137b4c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e137b4c",
        "outputId": "f1765eac-c4a5-4ecf-e9bb-1b8fae8f9459"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run sh: `/usr/bin/python3 /usr/local/lib/python3.11/dist-packages/swift/cli/sft.py --dataset Kvasir-VQA-x1/Kvasir-VQA-x1-train.jsonl --val_dataset Kvasir-VQA-x1/Kvasir-VQA-x1-test-1000.jsonl --model google/paligemma-3b-pt-224 --max_length 512 --train_type lora --torch_dtype float16 --quant_method bnb --quant_bits 4 --bnb_4bit_compute_dtype float16 --bnb_4bit_quant_type nf4 --bnb_4bit_use_double_quant true --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --lr_scheduler_type linear --warmup_ratio 0.03 --weight_decay 0.01 --lora_rank 16 --lora_alpha 32 --freeze_vit true --gradient_checkpointing true --load_best_model_at_end True --metric_for_best_model eval_token_acc --greater_is_better True --save_steps 1000 --save_total_limit 2 --logging_steps 20 --output_dir output_Kvasir-VQA-x1 --use_hf true --push_to_hub true --hub_model_id Kvasir-VQA-x1-lora_250812-1155 --report_to wandb --dataloader_num_workers 2 --dataset_num_proc 2`\n",
            "2025-08-12 11:57:48.368611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754999868.650195   59632 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754999868.732841   59632 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754999869.252275   59632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754999869.252318   59632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754999869.252323   59632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754999869.252331   59632 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-12 11:57:49.299111: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[INFO:swift] Successfully registered `/usr/local/lib/python3.11/dist-packages/swift/llm/dataset/data/dataset_info.json`.\n",
            "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: google/paligemma-3b-pt-224\n",
            "Fetching 11 files: 100% 11/11 [00:00<00:00, 118300.88it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--google--paligemma-3b-pt-224/snapshots/35e4f46485b4d07967e7e9935bc3786aad50687c\n",
            "[INFO:swift] Because len(args.val_dataset) > 0, setting split_dataset_ratio: 0.0\n",
            "[INFO:swift] Setting args.lazy_tokenize: True\n",
            "[INFO:swift] output_dir: /content/output_Kvasir-VQA-x1/v3-20250812-115800\n",
            "[INFO:swift] Global seed set to 42\n",
            "[INFO:swift] args: TrainArguments(\n",
            "_n_gpu=-1,\n",
            "acc_strategy=token,\n",
            "accelerator_config={'dispatch_batches': False},\n",
            "adafactor=False,\n",
            "adalora_beta1=0.85,\n",
            "adalora_beta2=0.85,\n",
            "adalora_deltaT=1,\n",
            "adalora_init_r=12,\n",
            "adalora_orth_reg_weight=0.5,\n",
            "adalora_target_r=8,\n",
            "adalora_tfinal=0,\n",
            "adalora_tinit=0,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.95,\n",
            "adam_epsilon=1e-08,\n",
            "adapter_act=gelu,\n",
            "adapter_length=128,\n",
            "adapters=[],\n",
            "add_version=True,\n",
            "agent_template=None,\n",
            "aligner_lr=None,\n",
            "attn_impl=None,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=True,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "bnb_4bit_compute_dtype=torch.float16,\n",
            "bnb_4bit_quant_storage=None,\n",
            "bnb_4bit_quant_type=nf4,\n",
            "bnb_4bit_use_double_quant=True,\n",
            "boft_block_num=0,\n",
            "boft_block_size=4,\n",
            "boft_dropout=0.0,\n",
            "boft_n_butterfly_factor=1,\n",
            "cached_dataset=[],\n",
            "channels=None,\n",
            "check_model=True,\n",
            "ckpt_dir=None,\n",
            "columns={},\n",
            "create_checkpoint_symlink=False,\n",
            "custom_dataset_info=[],\n",
            "custom_register_path=[],\n",
            "data_seed=42,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=2,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "dataset=['Kvasir-VQA-x1/Kvasir-VQA-x1-train.jsonl'],\n",
            "dataset_num_proc=2,\n",
            "dataset_shuffle=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=18000000,\n",
            "debug=None,\n",
            "deepspeed=None,\n",
            "deepspeed_autotp_size=None,\n",
            "device_map=None,\n",
            "disable_tqdm=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "download_mode=reuse_dataset_if_exists,\n",
            "ds3_gather_for_generation=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_dataset=[],\n",
            "eval_dataset_args=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_generation_config=None,\n",
            "eval_limit=None,\n",
            "eval_on_start=False,\n",
            "eval_steps=1000.0,\n",
            "eval_strategy=steps,\n",
            "eval_use_evalscope=False,\n",
            "eval_use_gather_object=False,\n",
            "external_plugins=[],\n",
            "fourier_n_frequency=2000,\n",
            "fourier_scaling=300.0,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "freeze_aligner=True,\n",
            "freeze_llm=False,\n",
            "freeze_parameters=[],\n",
            "freeze_parameters_ratio=0.0,\n",
            "freeze_parameters_regex=None,\n",
            "freeze_vit=True,\n",
            "fsdp=,\n",
            "fsdp_config=None,\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "galore_cos_threshold=0.4,\n",
            "galore_gamma_proj=2,\n",
            "galore_optim_per_parameter=False,\n",
            "galore_proj_bits=4,\n",
            "galore_proj_group_size=256,\n",
            "galore_proj_quant=False,\n",
            "galore_proj_type=std,\n",
            "galore_quantization=False,\n",
            "galore_queue_size=5,\n",
            "galore_rank=128,\n",
            "galore_scale=1.0,\n",
            "galore_target_modules=None,\n",
            "galore_update_proj_gap=50,\n",
            "galore_with_embedding=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=True,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hqq_axis=None,\n",
            "hub_always_push=False,\n",
            "hub_model_id=Kvasir-VQA-x1-lora_250812-1155,\n",
            "hub_private_repo=None,\n",
            "hub_revision=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_args_error=False,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "init_strategy=None,\n",
            "init_weights=True,\n",
            "interleave_prob=None,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "lazy_tokenize=True,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "liger_kernel_config=None,\n",
            "lisa_activated_layers=0,\n",
            "lisa_step_interval=20,\n",
            "llamapro_num_groups=None,\n",
            "llamapro_num_new_blocks=4,\n",
            "load_args=False,\n",
            "load_best_model_at_end=True,\n",
            "load_data_args=False,\n",
            "load_from_cache_file=True,\n",
            "local_rank=-1,\n",
            "local_repo_path=None,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/output_Kvasir-VQA-x1/v3-20250812-115800/runs,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=20,\n",
            "logging_strategy=steps,\n",
            "logprobs=False,\n",
            "lora_alpha=32,\n",
            "lora_bias=none,\n",
            "lora_dropout=0.05,\n",
            "lora_dtype=None,\n",
            "lora_ga_batch_size=2,\n",
            "lora_ga_direction=ArB2r,\n",
            "lora_ga_iters=2,\n",
            "lora_ga_max_length=1024,\n",
            "lora_ga_scale=stable,\n",
            "lora_ga_stable_gamma=16,\n",
            "lora_modules=[],\n",
            "lora_rank=16,\n",
            "lorap_lr_ratio=None,\n",
            "loss_scale=default,\n",
            "loss_type=None,\n",
            "lr_scheduler_kwargs=None,\n",
            "lr_scheduler_type=linear,\n",
            "max_epochs=None,\n",
            "max_grad_norm=1.0,\n",
            "max_length=512,\n",
            "max_memory={},\n",
            "max_model_len=None,\n",
            "max_new_tokens=64,\n",
            "max_pixels=None,\n",
            "max_steps=-1,\n",
            "metric=None,\n",
            "metric_for_best_model=eval_token_acc,\n",
            "model=google/paligemma-3b-pt-224,\n",
            "model_author=None,\n",
            "model_kwargs={},\n",
            "model_name=None,\n",
            "model_revision=None,\n",
            "model_type=paligemma,\n",
            "modules_to_save=[],\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "new_special_tokens=[],\n",
            "no_cuda=False,\n",
            "norm_bbox=None,\n",
            "num_beams=1,\n",
            "num_labels=None,\n",
            "num_train_epochs=1.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "optimizer=None,\n",
            "output_dir=/content/output_Kvasir-VQA-x1/v3-20250812-115800,\n",
            "overwrite_output_dir=False,\n",
            "packing=False,\n",
            "padding_free=False,\n",
            "padding_side=right,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "problem_type=None,\n",
            "push_to_hub=True,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "quant_bits=4,\n",
            "quant_method=bnb,\n",
            "ray_scope=last,\n",
            "reft_args=None,\n",
            "reft_intervention_type=LoreftIntervention,\n",
            "reft_layer_key=None,\n",
            "reft_layers=None,\n",
            "reft_rank=4,\n",
            "remove_unused_columns=True,\n",
            "repetition_penalty=None,\n",
            "report_to=['wandb'],\n",
            "response_prefix=None,\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "resume_only_model=False,\n",
            "rope_scaling=None,\n",
            "router_aux_loss_coef=None,\n",
            "run_name=/content/output_Kvasir-VQA-x1/v3-20250812-115800,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=1000.0,\n",
            "save_strategy=steps,\n",
            "save_total_limit=2,\n",
            "seed=42,\n",
            "sequence_parallel_size=1,\n",
            "shuffle_buffer_size=1000,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_dataset_ratio=0.0,\n",
            "stop_words=[],\n",
            "stopping_strategy=first_exhausted,\n",
            "stream=False,\n",
            "streaming=False,\n",
            "strict=False,\n",
            "swanlab_exp_name=None,\n",
            "swanlab_lark_secret=None,\n",
            "swanlab_lark_webhook_url=None,\n",
            "swanlab_mode=cloud,\n",
            "swanlab_project=None,\n",
            "swanlab_token=<SWANLAB_TOKEN>,\n",
            "swanlab_workspace=None,\n",
            "system=None,\n",
            "target_modules=['all-linear'],\n",
            "target_regex=None,\n",
            "task_type=causal_lm,\n",
            "temperature=0.0,\n",
            "template=paligemma,\n",
            "template_backend=swift,\n",
            "tf32=None,\n",
            "top_k=None,\n",
            "top_logprobs=None,\n",
            "top_p=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_dtype=torch.float16,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "train_dataloader_shuffle=True,\n",
            "train_type=lora,\n",
            "trainable_parameters=[],\n",
            "trainable_parameters_regex=None,\n",
            "truncation_strategy=delete,\n",
            "tuner_backend=peft,\n",
            "use_chat_template=True,\n",
            "use_cpu=False,\n",
            "use_dora=False,\n",
            "use_galore=False,\n",
            "use_hf=True,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_logits_to_keep=None,\n",
            "use_mps_device=False,\n",
            "use_rslora=False,\n",
            "use_swift_lora=False,\n",
            "val_dataset=['Kvasir-VQA-x1/Kvasir-VQA-x1-test-1000.jsonl'],\n",
            "val_dataset_shuffle=False,\n",
            "vera_d_initial=0.1,\n",
            "vera_dropout=0.0,\n",
            "vera_projection_prng_key=0,\n",
            "vera_rank=256,\n",
            "vit_gradient_checkpointing=None,\n",
            "vit_lr=None,\n",
            "warmup_ratio=0.03,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            "zero_hpz_partition_size=None,\n",
            ")\n",
            "[INFO:swift] Downloading the model from HuggingFace Hub, model_id: google/paligemma-3b-pt-224\n",
            "Fetching 14 files: 100% 14/14 [00:00<00:00, 15021.81it/s]\n",
            "[INFO:swift] Loading the model using model_dir: /root/.cache/huggingface/hub/models--google--paligemma-3b-pt-224/snapshots/35e4f46485b4d07967e7e9935bc3786aad50687c\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "[INFO:swift] model_kwargs: {'device_map': 'cuda:0', 'quantization_config': BitsAndBytesConfig {\n",
            "  \"_load_in_4bit\": true,\n",
            "  \"_load_in_8bit\": false,\n",
            "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "  \"bnb_4bit_quant_type\": \"nf4\",\n",
            "  \"bnb_4bit_use_double_quant\": true,\n",
            "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "  \"llm_int8_has_fp16_weight\": false,\n",
            "  \"llm_int8_skip_modules\": [\n",
            "    \"model.vision_tower\",\n",
            "    \"model.multi_modal_projector\",\n",
            "    \"lm_head\"\n",
            "  ],\n",
            "  \"llm_int8_threshold\": 6.0,\n",
            "  \"load_in_4bit\": true,\n",
            "  \"load_in_8bit\": false,\n",
            "  \"quant_method\": \"bitsandbytes\"\n",
            "}\n",
            "}\n",
            "Loading checkpoint shards: 100% 3/3 [01:03<00:00, 21.06s/it]\n",
            "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
            "[INFO:swift] model_info: ModelInfo(model_type='paligemma', model_dir='/root/.cache/huggingface/hub/models--google--paligemma-3b-pt-224/snapshots/35e4f46485b4d07967e7e9935bc3786aad50687c', torch_dtype=torch.float16, max_model_len=8192, quant_method='bnb', quant_bits=4, rope_scaling=None, is_moe_model=False, config=PaliGemmaConfig {\n",
            "  \"architectures\": [\n",
            "    \"PaliGemmaForConditionalGeneration\"\n",
            "  ],\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"hidden_size\": 2048,\n",
            "  \"ignore_index\": -100,\n",
            "  \"image_token_index\": 257152,\n",
            "  \"model_type\": \"paligemma\",\n",
            "  \"pad_token_id\": 0,\n",
            "  \"projection_dim\": 2048,\n",
            "  \"quantization_config\": {\n",
            "    \"_load_in_4bit\": true,\n",
            "    \"_load_in_8bit\": false,\n",
            "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
            "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
            "    \"bnb_4bit_quant_type\": \"nf4\",\n",
            "    \"bnb_4bit_use_double_quant\": true,\n",
            "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
            "    \"llm_int8_has_fp16_weight\": false,\n",
            "    \"llm_int8_skip_modules\": [\n",
            "      \"model.vision_tower\",\n",
            "      \"model.multi_modal_projector\",\n",
            "      \"lm_head\"\n",
            "    ],\n",
            "    \"llm_int8_threshold\": 6.0,\n",
            "    \"load_in_4bit\": true,\n",
            "    \"load_in_8bit\": false,\n",
            "    \"quant_method\": \"bitsandbytes\"\n",
            "  },\n",
            "  \"text_config\": {\n",
            "    \"attention_bias\": false,\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"head_dim\": 256,\n",
            "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "    \"hidden_activation\": null,\n",
            "    \"hidden_size\": 2048,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 16384,\n",
            "    \"max_position_embeddings\": 8192,\n",
            "    \"model_type\": \"gemma\",\n",
            "    \"num_attention_heads\": 8,\n",
            "    \"num_hidden_layers\": 18,\n",
            "    \"num_image_tokens\": 256,\n",
            "    \"num_key_value_heads\": 1,\n",
            "    \"rms_norm_eps\": 1e-06,\n",
            "    \"rope_theta\": 10000.0,\n",
            "    \"torch_dtype\": \"float16\",\n",
            "    \"use_cache\": true,\n",
            "    \"vocab_size\": 257216\n",
            "  },\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.55.0\",\n",
            "  \"vision_config\": {\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"hidden_act\": \"gelu_pytorch_tanh\",\n",
            "    \"hidden_size\": 1152,\n",
            "    \"image_size\": 224,\n",
            "    \"intermediate_size\": 4304,\n",
            "    \"layer_norm_eps\": 1e-06,\n",
            "    \"model_type\": \"siglip_vision_model\",\n",
            "    \"num_attention_heads\": 16,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 27,\n",
            "    \"num_image_tokens\": 256,\n",
            "    \"pad_token_id\": 0,\n",
            "    \"patch_size\": 14,\n",
            "    \"projection_dim\": 2048,\n",
            "    \"projector_hidden_act\": \"gelu_fast\",\n",
            "    \"torch_dtype\": \"float16\",\n",
            "    \"vision_use_head\": false\n",
            "  }\n",
            "}\n",
            ", task_type='causal_lm', num_labels=None)\n",
            "[INFO:swift] model.generation_config: GenerationConfig {\n",
            "  \"bos_token_id\": 2,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"max_new_tokens\": 64,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO:swift] default_system: None\n",
            "[INFO:swift] max_length: 512\n",
            "[INFO:swift] response_prefix: ''\n",
            "[INFO:swift] agent_template: react_en\n",
            "[INFO:swift] norm_bbox: norm1000\n",
            "[INFO:swift] Start time of running main: 2025-08-12 11:59:08.213105\n",
            "[INFO:swift] swift.__version__: 3.7.0\n",
            "Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 143594 examples [00:00, 255230.73 examples/s]\n",
            "Map (num_proc=2): 100% 143594/143594 [00:07<00:00, 19451.85 examples/s]\n",
            "Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "WARNING:datasets.builder:Setting num_proc from 2 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 1000 examples [00:00, 161493.30 examples/s]\n",
            "Map (num_proc=2): 100% 1000/1000 [00:00<00:00, 3557.24 examples/s]\n",
            "[INFO:swift] train_dataset: Dataset({\n",
            "    features: ['messages', 'images'],\n",
            "    num_rows: 143594\n",
            "})\n",
            "[INFO:swift] val_dataset: Dataset({\n",
            "    features: ['messages', 'images'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "[INFO:swift] [INPUT_IDS] [257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 257152, 2, 1841, 10286, 603, 47596, 575, 573, 2416, 235336, 108, 92323, 576, 11307, 111601, 10286, 1]\n",
            "[INFO:swift] [INPUT] [257152 * 256]<bos>What procedure is depicted in the image?\n",
            "evidence of colonoscopy procedure<eos>\n",
            "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 92323, 576, 11307, 111601, 10286, 1]\n",
            "[INFO:swift] [LABELS] [-100 * 266]evidence of colonoscopy procedure<eos>\n",
            "[INFO:swift] The TrainArguments will be saved in: /content/output_Kvasir-VQA-x1/v3-20250812-115800/args.json\n",
            "[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/root/.cache/huggingface/hub/models--google--paligemma-3b-pt-224/snapshots/35e4f46485b4d07967e7e9935bc3786aad50687c', revision=None, inference_mode=False, r=16, target_modules='^(model.language_model.*\\\\.(k_proj|gate_proj|q_proj|down_proj|up_proj|v_proj|o_proj))$', exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
            "[INFO:swift] model: PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): PaliGemmaForConditionalGeneration(\n",
            "      (model): PaliGemmaModel(\n",
            "        (vision_tower): SiglipVisionModel(\n",
            "          (vision_model): SiglipVisionTransformer(\n",
            "            (embeddings): SiglipVisionEmbeddings(\n",
            "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
            "              (position_embedding): Embedding(256, 1152)\n",
            "            )\n",
            "            (encoder): SiglipEncoder(\n",
            "              (layers): ModuleList(\n",
            "                (0-26): 27 x SiglipEncoderLayer(\n",
            "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "                  (self_attn): SiglipAttention(\n",
            "                    (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "                    (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "                    (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "                    (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
            "                  )\n",
            "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "                  (mlp): SiglipMLP(\n",
            "                    (activation_fn): PytorchGELUTanh()\n",
            "                    (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
            "                    (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (multi_modal_projector): PaliGemmaMultiModalProjector(\n",
            "          (linear): Linear(in_features=1152, out_features=2048, bias=True)\n",
            "        )\n",
            "        (language_model): GemmaModel(\n",
            "          (embed_tokens): Embedding(257216, 2048, padding_idx=0)\n",
            "          (layers): ModuleList(\n",
            "            (0-17): 18 x GemmaDecoderLayer(\n",
            "              (self_attn): GemmaAttention(\n",
            "                (q_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (k_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=256, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (v_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=256, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (o_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "              )\n",
            "              (mlp): GemmaMLP(\n",
            "                (gate_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=16384, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (up_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=16384, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (down_proj): lora.Linear4bit(\n",
            "                  (base_layer): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
            "                  (lora_dropout): ModuleDict(\n",
            "                    (default): Dropout(p=0.05, inplace=False)\n",
            "                  )\n",
            "                  (lora_A): ModuleDict(\n",
            "                    (default): Linear(in_features=16384, out_features=16, bias=False)\n",
            "                  )\n",
            "                  (lora_B): ModuleDict(\n",
            "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
            "                  )\n",
            "                  (lora_embedding_A): ParameterDict()\n",
            "                  (lora_embedding_B): ParameterDict()\n",
            "                  (lora_magnitude_vector): ModuleDict()\n",
            "                )\n",
            "                (act_fn): PytorchGELUTanh()\n",
            "              )\n",
            "              (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "              (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "            )\n",
            "          )\n",
            "          (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
            "          (rotary_emb): GemmaRotaryEmbedding()\n",
            "        )\n",
            "      )\n",
            "      (lm_head): Linear(in_features=2048, out_features=257216, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[INFO:swift] model_parameter_info: PeftModelForCausalLM: 1952.1738M Params (19.6116M Trainable [1.0046%]), 0.0004M Buffers.\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=2, read=1, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. (read timeout=0.5)\")': /api/v1/models/unknown/revisions\n",
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=2, read=0, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='www.modelscope.cn', port=443): Read timed out. (read timeout=0.5)\")': /api/v1/models/unknown/revisions\n",
            "/usr/local/lib/python3.11/dist-packages/swift/trainers/mixin.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  super().__init__(\n",
            "[INFO:swift] use_reentrant: True\n",
            "[INFO:swift] The logging file will be saved in: /content/output_Kvasir-VQA-x1/v3-20250812-115800/logging.jsonl\n",
            "[INFO:swift] Successfully registered post_encode hook: ['PeftModelForCausalLM'].\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msushantgautam\u001b[0m (\u001b[33mubl\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20250812_115927-qkc22fwd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/output_Kvasir-VQA-x1/v3-20250812-115800\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ubl/Kvasir-VQA-x1_Subtask1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ubl/Kvasir-VQA-x1_Subtask1/runs/qkc22fwd\u001b[0m\n",
            "Train:   0% 0/8975 [00:00<?, ?it/s][INFO:swift] use_logits_to_keep: False\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "{'loss': 3.49173903, 'token_acc': 0.32659933, 'grad_norm': 3.88380671, 'learning_rate': 7e-08, 'memory(GiB)': 11.34, 'train_speed(iter/s)': 0.083585, 'epoch': 0.0, 'global_step/max_steps': '1/8975', 'percentage': '0.01%', 'elapsed_time': '9s', 'remaining_time': '23h 38m 54s'}\n",
            "Train:   0% 15/8975 [01:41<16:43:21,  6.72s/it]"
          ]
        }
      ],
      "source": [
        "# training command\n",
        "# can also use full validation set in --val_dataset with \"VAL_PATH\"\n",
        "!swift sft \\\n",
        "--dataset \"$TRAIN_PATH\" \\\n",
        "--val_dataset \"$VAL_1000_PATH\" \\\n",
        "--model \"$MODEL_NAME\" \\\n",
        "--max_length 512 \\\n",
        "--train_type lora \\\n",
        "--torch_dtype float16 \\\n",
        "--quant_method bnb --quant_bits 4 \\\n",
        "--bnb_4bit_compute_dtype float16 \\\n",
        "--bnb_4bit_quant_type nf4 \\\n",
        "--bnb_4bit_use_double_quant true \\\n",
        "--num_train_epochs 1 \\\n",
        "--per_device_train_batch_size 4 \\\n",
        "--per_device_eval_batch_size 4 \\\n",
        "--gradient_accumulation_steps 4 \\\n",
        "--learning_rate 2e-5 \\\n",
        "--lr_scheduler_type linear \\\n",
        "--warmup_ratio 0.03 \\\n",
        "--weight_decay 0.01 \\\n",
        "--lora_rank 16 --lora_alpha 32 \\\n",
        "--freeze_vit true \\\n",
        "--gradient_checkpointing true \\\n",
        "--load_best_model_at_end True \\\n",
        "--metric_for_best_model eval_token_acc \\\n",
        "--greater_is_better True \\\n",
        "--save_steps 1000 \\\n",
        "--save_total_limit 2 \\\n",
        "--logging_steps 20 \\\n",
        "--output_dir output_Kvasir-VQA-x1 \\\n",
        "--use_hf true \\\n",
        "--push_to_hub true \\\n",
        "--hub_token  \"$(cat ~/.cache/huggingface/token)\" \\\n",
        "--hub_model_id \"$HUB_MODEL_ID\" \\\n",
        "--report_to wandb \\\n",
        "--dataloader_num_workers 2 \\\n",
        "--dataset_num_proc 2 \\\n",
        "# --resume_from_checkpoint output_Kvasir-VQA-x1/checkpoint-<LAST_STEP>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e5ce416",
      "metadata": {
        "id": "5e5ce416"
      },
      "source": [
        "\n",
        "## ğŸ”¬ Inference Sanity Check\n",
        "Load the LoRA-adapted model via `swift infer` on a couple of samples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd40d52a",
      "metadata": {
        "id": "cd40d52a"
      },
      "outputs": [],
      "source": [
        "from swift.llm import PtEngine, RequestConfig, InferRequest\n",
        "import json, random\n",
        "from PIL import Image\n",
        "\n",
        "import torch, gc # clean mem\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.ipc_collect()\n",
        "\n",
        "ADAPTERS = f\"{HF_USER}/{HUB_MODEL_ID}\"\n",
        "print(f\"Try to load model from: https://huggingface.co/{ADAPTERS} as an adapter to {MODEL_NAME}\")\n",
        "engine = PtEngine(model_id_or_path=MODEL_NAME, adapters=f\"{ADAPTERS}\", max_batch_size=2, use_hf=True, model_type=\"paligemma\")\n",
        "# adapters=XXXX should be  your huggingface repo saved from the training process above like \"SushantGautam/Kvasir-VQA-x1-lora-XXXX\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_SAMPLES = 10\n",
        "\n",
        "rcfg = RequestConfig(max_tokens=64, temperature=0)\n",
        "gc.collect(); torch.cuda.empty_cache(); torch.cuda.ipc_collect()\n",
        "\n",
        "choices = random.sample([json.loads(l) for l in open(VAL_PATH)], VAL_SAMPLES)\n",
        "reqs = [InferRequest(messages=[{'role':'user','content':f\"<image>{c['messages'][0]['content'].replace('<image>','').strip()}\"}],\n",
        "                     images=[c['images'][0]]) for c in choices]\n",
        "\n",
        "for c, r in zip(choices, engine.infer(reqs, rcfg)):\n",
        "    question = c['messages'][0]['content'].replace('<image>', '').strip()\n",
        "    real_answer = c['messages'][1]['content']\n",
        "    pred_answer = r.choices[0].message.content\n",
        "\n",
        "    print(\"\\nQ:\", question)\n",
        "    display(Image.open(c['images'][0]).resize((256,256)))\n",
        "    print(\"Pred:\", pred_answer, \"\\nReal:\", real_answer)"
      ],
      "metadata": {
        "id": "jxvE6nQk-Q7F"
      },
      "id": "jxvE6nQk-Q7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submitting to the competition\n",
        "To submit this model you have to add a new file named submission_task1.py in the root of your submission repo and need to edit that file with your details following the instructiosn at https://github.com/simula/MediaEval-Medico-2025/blob/main/README.md#-submission-system.\n",
        "\n"
      ],
      "metadata": {
        "id": "WxJ2Adz5_1L3"
      },
      "id": "WxJ2Adz5_1L3"
    },
    {
      "cell_type": "markdown",
      "id": "7d13b826",
      "metadata": {
        "id": "7d13b826"
      },
      "source": [
        "\n",
        "## ğŸ§  Tips & Tuning\n",
        "- If you hit **CUDA OOM**:\n",
        "  - Lower `per_device_train_batch_size` to 2 (or 1) and increase `gradient_accumulation_steps`.\n",
        "  - Lower `max_length` to 384 or 256.\n",
        "  - Ensure `freeze_vit=true` and `bnb` 4-bit is enabled.\n",
        "- If training is too slow, reduce dataset size temporarily for prototyping.\n",
        "- Increase `num_train_epochs` to 2â€“3 for better results if time allows.\n",
        "- For different VLMs, change `--model` to any supported multimodal model (see SWIFT docs).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2bee7e",
      "metadata": {
        "id": "6e2bee7e"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "### âœ… Youâ€™re done!\n",
        "You can now use your pushed model in other notebooks or pipelines, or extend this setup for **Subtask 2** (explanations) by adding structured outputs (text / visual evidence). Good luck! ğŸ€\n",
        "\n",
        "\n",
        "Dont hesitate to contact the organizers for any questiosn or help.\n",
        "https://github.com/simula/MediaEval-Medico-2025/blob/main/README.md#-organizers\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}